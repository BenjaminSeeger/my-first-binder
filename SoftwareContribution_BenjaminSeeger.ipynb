{"metadata":{"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task2 Benjamin Seeger\nIn this file I am working on task 2 of the lecture. Here my model can be developed. More detailed explanations are found in the final report. As my birthday is in February, I have been assigned the following focus:\n1. Performance\n2. Explainability\n3. Data Analysis  \n> Accordingly, in the following implementation, efforts will be made primarily to reduce loss and increase accuracy. According to the prioritisation, the other aspects will of course also be considered. The Explainability point is especially implemented through the additional text explaining the code. At the bottom is an archive of models that reflect the development of my network. In each case, the results of the network that influenced the development have been analysed. ","metadata":{"tags":[]},"id":"1dad66b6-ad6e-4ce2-899e-57ea5c3bc5f5"},{"cell_type":"code","source":"# Test if the binder works\nprint(\"Hello to my task.\")","metadata":{},"execution_count":null,"outputs":[],"id":"895df391-7d8e-4c2c-971d-53a119963036"},{"cell_type":"markdown","source":"# Implementation\nIn the following, the task is processed with the MNIST dataset. Individual subheadings that structure the code are intended to provide an overview. Notes on the implementation and justifications for the code are inserted at the appropriate places.\n## Import of the MNIST-Dataset\nThe following lines load the dataset and import other things that are helpful for later analyses, for example.","metadata":{},"id":"c2f3649b-deea-4fc5-bc7b-897416d2638b"},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_validation, y_validation) = mnist.load_data()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8033dcf8-6928-4df7-b62c-d8ed5a89cc36"},{"cell_type":"markdown","source":"Here we can print the first element of the training dataset. This will give us later a reference for the pre-processing process.","metadata":{},"id":"7f1327a4-8e29-477c-92a0-5353a5a1eb29"},{"cell_type":"code","source":"plt.imshow(x_train[0], cmap = plt.cm.binary)","metadata":{},"execution_count":null,"outputs":[],"id":"0ba795c7-25e1-4ee9-81d9-9f3fe00c36c3"},{"cell_type":"markdown","source":"## Pre-processing\nBy preparing the data, the training process can be accelerated by scaling the data from the greyscale values 0 to 255 to 0 to 1. Later on this operations will end in a loss decreasment an so on a accuracy improvement.","metadata":{},"id":"7eb2e8f0-37a7-4c5c-abc8-c4d8fd74c251"},{"cell_type":"code","source":"x_train = tf.keras.utils.normalize(x_train, axis = 1)\nx_validation = tf.keras.utils.normalize(x_validation, axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"3dc0e75d-773c-4724-a634-a3a27ef16fb5"},{"cell_type":"markdown","source":"By printing the first element of the training set again you can see how the greyscale valuas were fitted in the range of 0 to 1 in comparishment to the print before.","metadata":{},"id":"1dede662-3fba-4ebc-904e-4690c9224d4b"},{"cell_type":"code","source":"plt.imshow(x_train[0], cmap = plt.cm.binary)","metadata":{},"execution_count":null,"outputs":[],"id":"8b80cdb7-9d07-4016-862d-5f4c2a11bff8"},{"cell_type":"markdown","source":"Reshaping the data is needed for the input layer.","metadata":{},"id":"eecf0e44-fa9f-4166-a359-e5ec87b5f733"},{"cell_type":"code","source":"x_train = x_train.reshape(-1, 28, 28, 1)\nx_validation = x_validation.reshape(-1, 28, 28, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"6bd8eec0-ce39-4478-87c2-cfeb189a0f71"},{"cell_type":"markdown","source":"## Definition of the model architecture\nIn the following code box my model is defined and compiled. If you want you can get a summary in the last line of the box as well. More detailed information to the development process are found in the last section in the archive. There are some of my models I tried. Som more informations about my final model here are in the final report.","metadata":{},"id":"a420723c-81d6-40ff-87a7-22d94ecff30f"},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# If the summary of the network is desired, comment out the following line\n#model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2c717005-f3dd-48c2-a3ec-48e3172a1490"},{"cell_type":"markdown","source":"## Train the network\nSee the following code.","metadata":{},"id":"cf99ef23-dd66-4a29-9b6f-db5d6da65633"},{"cell_type":"code","source":"import datetime\nimport os\nlogdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n\nmodel.fit(x_train, \n          y_train, \n          epochs=10,\n          callbacks=[tensorboard_callback],\n          validation_data=(x_validation, y_validation)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"4b954c7e-939b-492f-8926-c7bfd7118ee6"},{"cell_type":"markdown","source":"## Evaluate the network\nThe following code evaluates the trained model. This numbers are copied to each of the development steps listed in the archive.","metadata":{},"id":"37d30c78-64f4-4bb8-9a7b-ad552efc2f4f"},{"cell_type":"code","source":"_, validation_acc = model.evaluate(x_validation, y_validation)\nprint('validation accuracy:', validation_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"35d9d124-00a4-45b9-94aa-092910d3b9ed"},{"cell_type":"markdown","source":"Here you can look at the predictions of the model. Green ones are right and red false ones. When you analyse the output, you can see, that some of the false predicted ones are very bad numbers. Those ones cannot be predicted. This code is from the DeepDive.","metadata":{},"id":"80794b93-6b53-4267-95d6-08dda4c71f79"},{"cell_type":"code","source":"import math\n\npredictions = model.predict([x_validation])\npredictions = np.argmax(predictions, axis=1)\n    \nnumbers_to_display = 400\nnum_cells = math.ceil(math.sqrt(numbers_to_display))\nplt.figure(figsize=(15, 15))\n\nfor plot_index in range(numbers_to_display): \n    predicted_label = predictions[plot_index]\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    color_map = 'Greens' if predicted_label == y_validation[plot_index] else 'Reds'\n    plt.subplot(num_cells, num_cells, plot_index + 1)\n    plt.imshow(x_validation[plot_index].reshape((28, 28)), cmap=color_map)\n    plt.xlabel(predicted_label)\n\nplt.subplots_adjust(hspace=1, wspace=0.5)\nplt.show()","metadata":{},"execution_count":null,"outputs":[],"id":"0b0b8a60-b06e-499f-bc1f-f4392dad9cdb"},{"cell_type":"markdown","source":"The following code prints a graph which is useful to analyse the epochs of the training process.","metadata":{},"id":"597e4373-ce3f-4aeb-94c4-67989f85882f"},{"cell_type":"code","source":"#Hier so ne Matlab Grafik ausgeben lassen...","metadata":{},"execution_count":null,"outputs":[],"id":"42b4e4ea-9124-4244-a203-5a45b1729d3c"},{"cell_type":"markdown","source":"## Archive\nHere are listed some of the models I tried. Of course I tried and rated a lot more, but the following steps had the most significant changes in the development process of the model. Also a short comment shows the idea behind the next step. Every model was trained in ten epochs therefore the key figures are comparable.","metadata":{},"id":"abcb7870-2aa1-44b0-a306-15e56b553dbf"},{"cell_type":"code","source":"#1: Just a basic structure to start with. At the end there are 10 numbers, so 10 neurons at the end.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#1: loss: 0.1056 - accuracy: 0.9734 - validation accuracy: 0.9733999967575073\n\n#2: Tryout if more neurons are helping the process.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#2: loss: 0.1099 - accuracy: 0.9762 - validation accuracy: 0.9761999845504761\n\n#3: Accuracy was improved a little bit, but the loss not. Although I wanted to try with 28x28=784 in the first layer and then compaction.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(784, activation=tf.nn.relu),\n    tf.keras.layers.Dense(392, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#3: loss: 0.1013 - accuracy: 0.9782 - validation accuracy: 0.9782000184059143\n\n#4: This adjustments helped not that much. The next idea is to replace the first dense layer with a convolutional followed by a MaxPooling layer and flatten later.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"relu\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#4: loss: 0.0586 - accuracy: 0.9853 - validation accuracy: 0.9853000044822693\n\n#5: This looks like the way to go. As you can see the loss and accuracy are showing a huge performance improvement of the model. Next try is the activation function of the dense layer \nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#5: loss: 0.0558 - accuracy: 0.9848 - validation accuracy: 0.9847999811172485\n\n#6: The loss was slightly improved in exchange for a little bit of accuracy. The analyses of the epochs nevertheless showed that the values were reached more quickly, however, which is why the adjustment counts as an improvement.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#6: loss: 0.0441 - accuracy: 0.9855 - validation accuracy: 0.9854999780654907\n\n#7: The first three layers have improved the model even more. Perhaps this constellation will bring another improvement as well.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#7: loss: 0.0345 - accuracy: 0.9885 - validation accuracy: 0.9884999990463257\n\n#8: What just worked could possibly help again. \nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#8: loss: 0.1048 - accuracy: 0.9694 - validation accuracy: 0.9693999886512756\n\n#9: Unfortunately not. That was probably too much. Back to #7 and adjust the first layer so that the mesh is condensed piece by piece at the beginning.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#9: loss: 0.0318 - accuracy: 0.9897 - validation accuracy: 0.9897000193595886\n\n#10: That was a success!! The \"funneling\" at the beginning has helped, perhaps the expansion of the \"funnel\" can increase the performance.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#10: loss: 0.1492 - accuracy: 0.9465 - validation accuracy: 0.9275638756478595\n\n#11: Very bad idea!!! Maybe the \"funnel\" was just extended in the wrong direction.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#11: loss: 0.1047 - accuracy: 0.9385 - validation accuracy: 0.9395752574558456\n\n#12: Nope. Bad again. #9 is still my best achitecture. Since the further \"funneling\" before the flattening layer didn't help any more, here's the attempt to continue this idea afterwards\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(64, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#12: loss: 0.0351 - accuracy: 0.9889 - validation accuracy: 0.9889000058174133\n\n#13: Still #9 is the best one. At this point i tried a lot, add new layers and different layers and so on. Nothing I did improved my model why I wanted to try some things at the model.compile.\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#13:loss: 0.1355 - accuracy: 0.9587 - validation accuracy: 0.9587000012397766\n\n\n\n#As well I tried a lot more as the shown one but unfortunately without success. From all things I tried #9 is still the best.\n#Unfortunately, I could not try more due to time constraints, which is why #9 is used as my final model at this point.\n\n#final: Here is my final modell:\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=\"sigmoid\"),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n#final: loss: 0.0318 - accuracy: 0.9897 - validation accuracy: 0.9897000193595886","metadata":{},"execution_count":null,"outputs":[],"id":"d81ed204-cb76-40ad-8383-06bb037fe748"}]}